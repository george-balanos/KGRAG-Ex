{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9486fc1b",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb8a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_RAG_utilities import *\n",
    "from config import prompt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, model, vector_store = setup_simple_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples, ground_truth_answers = prepare_benchmark_data(dataset_name=\"medmcqa\")\n",
    "\n",
    "# for example, answer in zip(test_examples, ground_truth_answers):\n",
    "#     print(f\"Example: {example}\\nAnswer: {answer}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23113766",
   "metadata": {},
   "source": [
    "Naive RAG Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc27e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_answers = []\n",
    "precision = 0.0\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "for example in tqdm(test_examples, desc=\"Generating Answers\", total=len(test_examples)):\n",
    "\n",
    "    simple_rag_answers.append(run_simple_rag(chain, example[0], example[1], vector_store))\n",
    "\n",
    "for i in range(len(simple_rag_answers)):\n",
    "    if simple_rag_answers[i] == ground_truth_answers[i]:\n",
    "        precision += 1\n",
    "\n",
    "precision = precision / len(simple_rag_answers)\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_responses = []\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "temp_dict = {}\n",
    "\n",
    "for index, example in tqdm(enumerate(test_examples), desc=\"Generating Responses\", total=len(test_examples)):\n",
    "    question = example[0]\n",
    "    options = example[1]\n",
    "\n",
    "    context = retrieve_relevant_document(question, vector_store)\n",
    "    context_text = context[0].page_content.capitalize()\n",
    "    original_response = generate_response(chain, {\"paragraph\": context_text, \"question\": question, \"options\": options})\n",
    "\n",
    "    if original_response in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        temp_dict[f\"question_{index}\"] = { \"original_response\": original_response }\n",
    "        perturbations = remove_word_span(context_text, 5)\n",
    "\n",
    "        for per in tqdm(perturbations, desc=f\"Processing Perturbations (Test Example: {index})\", total=len(perturbations)):\n",
    "            perturbed_text = per[0]\n",
    "            removed_token = per[1]\n",
    "            position = per[2]\n",
    "\n",
    "            temp_response = generate_response(chain, {\"paragraph\": perturbed_text, \"question\": question, \"options\": options})\n",
    "\n",
    "            if temp_response != original_response:\n",
    "                temp_dict[f\"question_{index}\"][f\"perturbation_{position}\"] = {\"perturbed_text\": perturbed_text, \"removed_token(critical)\": removed_token, \"answer\": temp_response}\n",
    "\n",
    "with open(r\"../results/simple_rag_result_per_word_5.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(temp_dict, json_file, indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677bb3e",
   "metadata": {},
   "source": [
    "**Comparison of Exhaustive (RAG-Ex) Approach (span_size: 5) and KGRAG-Ex: LLM Calls and Token Count for**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(r\"../results/mmlu_calls_amount.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    kg_rag = json.load(json_file)\n",
    "\n",
    "with open(r\"../results/mmlu_calls_amount_simple.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    simple_rag = json.load(json_file)\n",
    "\n",
    "example_comparisons = {}\n",
    "\n",
    "for test in kg_rag:\n",
    "    example_comparisons[f\"comparison_of_{test}\"] = {\"simple\": simple_rag[test], \"kg\": kg_rag[test]}\n",
    "\n",
    "with open(r\"../results/mmlu_calls_amount_comparison.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(example_comparisons, json_file, indent=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = example_comparisons\n",
    "\n",
    "llm_calls_simple = []\n",
    "llm_calls_kg = []\n",
    "\n",
    "tokens_simple = []\n",
    "tokens_kg = []\n",
    "\n",
    "for key, values in data.items():\n",
    "    simple = values[\"simple\"]\n",
    "    kg = values[\"kg\"]\n",
    "\n",
    "    if \"llm_calls\" in simple and \"llm_calls\" in kg:\n",
    "        llm_calls_simple.append(simple[\"llm_calls\"])\n",
    "        llm_calls_kg.append(kg[\"llm_calls\"])\n",
    "    else:\n",
    "        print(f\"Missing llm_calls data for {key}\")\n",
    "\n",
    "    if \"total_tokens\" in simple and \"total_tokens\" in kg:\n",
    "        tokens_simple.append(simple[\"total_tokens\"])\n",
    "        tokens_kg.append(kg[\"total_tokens\"])\n",
    "    else:\n",
    "        print(f\"Missing total_tokens data for {key}\")\n",
    "\n",
    "median_llm_simple = np.median(llm_calls_simple)\n",
    "median_llm_kg = np.median(llm_calls_kg)\n",
    "diff_llm = median_llm_simple - median_llm_kg\n",
    "\n",
    "median_tokens_simple = np.median(tokens_simple)\n",
    "median_tokens_kg = np.median(tokens_kg)\n",
    "diff_tokens = median_tokens_simple - median_tokens_kg\n",
    "\n",
    "print(\"\\n--- Median Summary ---\")\n",
    "print(f\"{'Metric':<20} {'Simple':>10} {'KG':>10} {'Diff (S - KG)':>15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'LLM Calls':<20} {median_llm_simple:>10.1f} {median_llm_kg:>10.1f} {diff_llm:>15.1f}\")\n",
    "print(f\"{'Total Tokens':<20} {median_tokens_simple:>10.1f} {median_tokens_kg:>10.1f} {diff_tokens:>15.1f}\")\n",
    "\n",
    "llm_calls_diff_raw = [s - k for s, k in zip(llm_calls_simple, llm_calls_kg)]\n",
    "tokens_diff_raw = [s - k for s, k in zip(tokens_simple, tokens_kg)]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(llm_calls_diff_raw, marker='o', linestyle='-', color='blue')\n",
    "# plt.axhline(diff_llm, color='gray', linestyle='--', label='Median Difference')\n",
    "# plt.title('Raw Difference in LLM Calls (Simple - KG)')\n",
    "# plt.ylabel('Difference')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(tokens_diff_raw, marker='o', linestyle='-', color='green')\n",
    "# plt.axhline(diff_tokens, color='gray', linestyle='--', label='Median Difference')\n",
    "# plt.title('Raw Difference in Total Tokens (Simple - KG)')\n",
    "# plt.xlabel('Example Index')\n",
    "# plt.ylabel('Difference')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
