{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3486a4",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2302b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd21cd9",
   "metadata": {},
   "source": [
    "Load KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55074242",
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_path = r\"../data/arch_kg.csv\"\n",
    "\n",
    "df = pd.read_csv(KG_path)\n",
    "for _,row in df.head(5).iterrows():\n",
    "    print(f'{row[\"Entity1\"]} ({row[\"Label1\"]})->[{row[\"Relationship\"]}]->{row[\"Entity2\"]} ({row[\"Label2\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e920be8",
   "metadata": {},
   "source": [
    "Create DiGraph (Knowledge Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    G.add_node(row[\"Entity1\"], label=row[\"Label1\"])\n",
    "    G.add_node(row[\"Entity2\"], label=row[\"Label2\"])\n",
    "\n",
    "    G.add_edge(\n",
    "        row[\"Entity1\"],\n",
    "        row[\"Entity2\"],\n",
    "        relationship=row[\"Relationship\"],\n",
    "        source=row[\"SourceFilename\"],\n",
    "        chunk=row[\"ChunkID\"]\n",
    "    )\n",
    "\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} egdes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48154247",
   "metadata": {},
   "source": [
    "Create MMLU Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "benchmark_path = r\"../data/benchmark.json\"\n",
    "\n",
    "with open(benchmark_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "mmlu_data = data[\"mmlu\"]\n",
    "\n",
    "flat_data = []\n",
    "for qid, qdata in mmlu_data.items():\n",
    "    flat_data.append({\n",
    "        \"id\": qid,\n",
    "        \"question\": qdata[\"question\"],\n",
    "        \"options\": qdata[\"options\"],\n",
    "        \"answer\": qdata[\"answer\"]\n",
    "    })\n",
    "\n",
    "mmlu_df = pd.DataFrame(flat_data)\n",
    "print(mmlu_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949e137",
   "metadata": {},
   "source": [
    "Create MedMCQA Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "benchmark_path = r\"../data/benchmark.json\"\n",
    "\n",
    "with open(benchmark_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "medmcqa_data = data[\"medmcqa\"]\n",
    "\n",
    "flat_data = []\n",
    "for qid, qdata in medmcqa_data.items():\n",
    "    flat_data.append({\n",
    "        \"id\": qid,\n",
    "        \"question\": qdata[\"question\"],\n",
    "        \"options\": qdata[\"options\"],\n",
    "        \"answer\": qdata[\"answer\"]\n",
    "    })\n",
    "\n",
    "medmcqa_df = pd.DataFrame(flat_data)\n",
    "print(medmcqa_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb81b48",
   "metadata": {},
   "source": [
    "Pydantic Related Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Tuple\n",
    "\n",
    "class EntityPair(BaseModel):\n",
    "    entity1: str = Field(...)\n",
    "    entity2: str = Field(...)\n",
    "\n",
    "    @field_validator(\"entity1\", \"entity2\")\n",
    "    def not_empty(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"Entity cannot be empty\")\n",
    "        return v.strip()\n",
    "\n",
    "def preprocess_line(line: str) -> str:\n",
    "    line = line.strip()\n",
    "\n",
    "    if line.startswith(\"*\"):\n",
    "        line = line.lstrip(\"*\").strip()\n",
    "\n",
    "    if line.startswith(\"(\") and line.endswith(\")\"):\n",
    "        line = line[1:-1].strip()\n",
    "\n",
    "    if (line.startswith(\"'\") and line.endswith(\"'\")) or (line.startswith('\"') and line.endswith('\"')):\n",
    "        line = line[1:-1].strip()\n",
    "\n",
    "    return line\n",
    "\n",
    "def clean_entity(entity: str) -> str:\n",
    "    entity = entity.strip()\n",
    "    while entity.startswith((\"'\", '\"')) or entity.endswith((\"'\", '\"')):\n",
    "        if entity.startswith((\"'\", '\"')):\n",
    "            entity = entity[1:]\n",
    "        if entity.endswith((\"'\", '\"')):\n",
    "            entity = entity[:-1]\n",
    "        entity = entity.strip()\n",
    "    return entity\n",
    "\n",
    "def parse_pairs(raw_text: str) -> List[EntityPair]:\n",
    "    lines = raw_text.strip().splitlines()\n",
    "    pairs = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = preprocess_line(line)\n",
    "\n",
    "        if not line or \",\" not in line:\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\",\", 1)\n",
    "        if len(parts) != 2:\n",
    "            raise ValueError(f\"Invalid pair format: {line}\")\n",
    "        \n",
    "        entity1 = clean_entity(parts[0])\n",
    "        entity2 = clean_entity(parts[1])\n",
    "\n",
    "        if not entity1 or not entity2:\n",
    "            print(f\"Skipping invalid pair with empty entity: '{entity1}', '{entity2}'\")\n",
    "            continue\n",
    "\n",
    "        pair = EntityPair(entity1=entity1, entity2=entity2)\n",
    "        pairs.append(pair)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68ab9b",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ff73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_except_underscores(lst):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if item == \"__\":\n",
    "            result.append(item)\n",
    "        elif item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def format_edge_path(path):\n",
    "    formatted = \"\"\n",
    "    for i in range(0, len(path) - 2, 2):\n",
    "        source = path[i]\n",
    "        relation = path[i + 1]\n",
    "        target = path[i + 2]\n",
    "        formatted += f\"{source}->[{relation}]->\"\n",
    "    formatted += path[-1]  # Add last node\n",
    "    return formatted\n",
    "\n",
    "def remove_direct_neighbors_only(data):\n",
    "    indices_to_remove = set()\n",
    "    for i, val in enumerate(data):\n",
    "        if val == '__':\n",
    "            if i > 0 and data[i - 1] != '__':\n",
    "                indices_to_remove.add(i - 1)\n",
    "            if i < len(data) - 1 and data[i + 1] != '__':\n",
    "                indices_to_remove.add(i + 1)\n",
    "    return [val for i, val in enumerate(data) if i not in indices_to_remove]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc6699",
   "metadata": {},
   "source": [
    "RAG and Perturbation Methods Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce29201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\n",
    "    You are given the following question in a medical context:\n",
    "                                          \n",
    "        {question}\n",
    "                                          \n",
    "    Your task is to extract medically relevant entity pairs from the text.\n",
    "    Return the output strictly in the format:\n",
    "        \n",
    "        <entity1, entity2>\n",
    "                                          \n",
    "    Make sure each pair is unique and avoid repeating the same entity in both positions.\n",
    "                                          \n",
    "    Do not explain or add anything else - just return the entity pairs.\n",
    "''')\n",
    "\n",
    "prompt_pseudo_paragraph = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a medical instructor helping to generate educational materials.\n",
    "                                                    \n",
    "Given a relationship chain connecting medical concepts, write a short, clear and medically accurate paragraph that explains the connections in a way understandable to students.\n",
    "                                                           \n",
    "Input chain:\n",
    "{relation_chain}\n",
    "                                                           \n",
    "Output:\n",
    "A single, coherent paragraph explaining the relationships. Do not include any further comments, only your generated answer.\n",
    "\"\"\")\n",
    "\n",
    "RAG_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a knowledgeable medical assistant.\n",
    "\n",
    "Use the following medical paragraph to answer the multiple-choice question below.\n",
    "Choose the best answer from the provided options based solely on the paragraph.\n",
    "                                              \n",
    "Medical Paragraph:\n",
    "{paragraph}\n",
    "                                              \n",
    "Question:\n",
    "{question}\n",
    "                                              \n",
    "Options:\n",
    "{options}\n",
    "                                              \n",
    "Answer:\n",
    "Provide only the letter (A, B, C or D) corresponding to the corrected choice.\n",
    "Do not provide any additional explanation or commentary.\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | model\n",
    "chain_par = prompt_pseudo_paragraph | model\n",
    "rag_chain = RAG_prompt | model \n",
    "\n",
    "def extract_entities(question):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    pairs = parse_pairs(response)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def find_shortest_path(entity1, entity2):\n",
    "    path = \"\"\n",
    "    node_list = []\n",
    "    edge_list = []\n",
    "    subpath_list = []\n",
    "\n",
    "    try:\n",
    "        shortest_path = nx.shortest_path(G, entity1, entity2)\n",
    "        # print(f\"Shortest path: \", shortest_path)\n",
    "\n",
    "        for i in range(len(shortest_path) - 1):\n",
    "            u = shortest_path[i]\n",
    "            v = shortest_path[i+1]\n",
    "            edge_data = G.get_edge_data(u, v)\n",
    "            relationship = edge_data.get(\"relationship\",\"unknown\") if edge_data else \"no_relationship\"\n",
    "            # print(f\"{u}->[{relationship}]->{v}\")\n",
    "\n",
    "            path += f\"{u}->[{relationship}]->\"\n",
    "\n",
    "            node_list.append(u)\n",
    "            edge_list.append(relationship)\n",
    "            subpath_list.append((u, relationship, v))\n",
    "            \n",
    "        node_list.append(shortest_path[-1])\n",
    "        path += f\"{shortest_path[-1]}\"\n",
    "    except nx.NetworkXNoPath:\n",
    "        # print(f\"No path found between {entity1} and {entity2}!\")\n",
    "        return\n",
    "    except nx.NodeNotFound as e:\n",
    "        # print(f\"Node not found error: {e}\")\n",
    "        return\n",
    "\n",
    "    # print(f\"Shortest path: {path}\")\n",
    "    return [node_list, edge_list, subpath_list, path]\n",
    "\n",
    "def generate_pseudo_paragraph(path):\n",
    "    paragraph = chain_par.invoke({\"relation_chain\": path})\n",
    "    # print(f\"Paragraph: {paragraph}\")\n",
    "\n",
    "    return paragraph\n",
    "\n",
    "def run_rag(paragraph, question, options):\n",
    "    response = rag_chain.invoke({\"paragraph\": paragraph, \"question\": question, \"options\": options})\n",
    "    return response[0]\n",
    "\n",
    "def generate_perturbations(node_list, edge_list, subpath_list):\n",
    "    node_perturbations = []\n",
    "    edge_perturbations = []\n",
    "    subpath_perturbations = []\n",
    "\n",
    "    for i in range(len(node_list)):\n",
    "        perturbation = node_list[:i]+[\"__\"]+node_list[i+1:]\n",
    "\n",
    "        temp = \"\"\n",
    "        for j in range(len(node_list)-1):\n",
    "            temp += f\"{perturbation[j]}->[{edge_list[j]}]->\"\n",
    "        \n",
    "        temp += perturbation[-1]\n",
    "        node_perturbations.append((temp, node_list[i], G.nodes[node_list[i]].get(\"label\")))\n",
    "\n",
    "    for i in range(len(edge_list)):\n",
    "        perturbation = edge_list[:i]+[\"__\"]+edge_list[i+1:]\n",
    "\n",
    "        temp = \"\"\n",
    "        for j in range(len(node_list)-1):\n",
    "            temp += f\"{node_list[j]}->[{perturbation[j]}]->\"\n",
    "\n",
    "        temp += node_list[-1]\n",
    "        edge_perturbations.append(temp)\n",
    "\n",
    "    for i in range(len(subpath_list)):\n",
    "        perturbation = subpath_list[:i]+[(\"__\",\"__\",\"__\")]+subpath_list[i+1:]\n",
    "\n",
    "        elements = []\n",
    "        for p in perturbation:\n",
    "            elements.append(p[0])\n",
    "            elements.append(p[1])\n",
    "            elements.append(p[2])\n",
    "\n",
    "        elements = remove_duplicates_except_underscores(elements)\n",
    "        elements = remove_direct_neighbors_only(elements)\n",
    "        temp = format_edge_path(elements)\n",
    "\n",
    "        subpath_perturbations.append(temp)\n",
    "\n",
    "    return {\n",
    "        \"node_perturbations\": node_perturbations,\n",
    "        \"edge_perturbations\": edge_perturbations,\n",
    "        \"subpath_perturbations\": subpath_perturbations\n",
    "        }\n",
    "\n",
    "def produce_default_answer(question, options):\n",
    "    \n",
    "    entity_pairs = extract_entities(question)\n",
    "    entity_pairs = [(pair.entity1, pair.entity2) for pair in entity_pairs]\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    for pair in entity_pairs:\n",
    "        shortest_path = find_shortest_path(pair[0], pair[1])\n",
    "        if shortest_path:\n",
    "            paths.append(shortest_path[3])\n",
    "    \n",
    "    # print(f\"Default paths:\\n\")\n",
    "    paragraphs = []\n",
    "    for p in paths:\n",
    "        # print(f\"\\t{p}\")\n",
    "        par = generate_pseudo_paragraph(p)\n",
    "        paragraphs.append(par)\n",
    "\n",
    "    paragraphs = \"\\n\".join(paragraphs)\n",
    "    # print(f\"\\nDefault paragraphs: {paragraphs}\\n\")\n",
    "\n",
    "    response = run_rag(paragraphs, question, options)\n",
    "    # print(f\"Default Response: {response}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # print(\"#\"*40)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    return response, len(paragraphs.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de740d6",
   "metadata": {},
   "source": [
    "Application of Perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47376f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "\n",
    "for test_idx,example in tqdm(list(medmcqa_df.iterrows()), desc=\"Apply Perturbations\", total=len(list(medmcqa_df.iterrows()))):\n",
    "\n",
    "    node_counter = 0\n",
    "    edge_counter = 0\n",
    "    subpath_counter = 0\n",
    "\n",
    "    label_list = []\n",
    "\n",
    "    node_positions = []\n",
    "    edge_positions = []\n",
    "    subpath_positions = []\n",
    "\n",
    "    question = example[\"question\"]\n",
    "    options = example[\"options\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    default_response, _ = produce_default_answer(question, options)\n",
    "    # print(default_response)\n",
    "\n",
    "    path_lists = []\n",
    "    paths = []\n",
    "\n",
    "    entity_pairs = extract_entities(question)\n",
    "    entity_pairs = [(pair.entity1, pair.entity2) for pair in entity_pairs]\n",
    "    # print(f\"Entity Pairs: {entity_pairs}\\n\")\n",
    "\n",
    "    for pair in entity_pairs:\n",
    "        shortest_path = find_shortest_path(pair[0], pair[1])\n",
    "        if shortest_path:\n",
    "            path_lists.append(shortest_path[:3])\n",
    "            paths.append(shortest_path[3])\n",
    "\n",
    "    # print()\n",
    "    # print(f\"Paths:\\n\")\n",
    "    # for path in paths:\n",
    "    #     print(f\"\\t{path}\")\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    if len(paths) == 0:\n",
    "        continue\n",
    "\n",
    "    responses[f\"test_{test_idx}\"] = {}\n",
    "    responses[f\"test_{test_idx}\"][\"node\"] = {}\n",
    "    responses[f\"test_{test_idx}\"][\"edge\"] = {}\n",
    "    responses[f\"test_{test_idx}\"][\"subpath\"] = {}\n",
    "\n",
    "    all_perturbations = []\n",
    "\n",
    "    for i in range(len(paths)):\n",
    "        perturbations = generate_perturbations(path_lists[i][0], path_lists[i][1], path_lists[i][2])\n",
    "        all_perturbations.append(perturbations)\n",
    "\n",
    "\n",
    "    for index, perturbation in enumerate(all_perturbations):\n",
    "        node_perturbations = perturbation[\"node_perturbations\"]\n",
    "        edge_perturbations = perturbation[\"edge_perturbations\"]\n",
    "        subpath_perturbations = perturbation[\"subpath_perturbations\"]\n",
    "\n",
    "        # print(f\"Node perturbations\")\n",
    "\n",
    "        for i, perturbation in enumerate(node_perturbations):\n",
    "            temp_paths = paths[:]\n",
    "            temp_paths.pop(index)\n",
    "            \n",
    "            temp_paths.insert(index, perturbation[0])\n",
    "\n",
    "            # print(temp_paths)\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in temp_paths:\n",
    "                par = generate_pseudo_paragraph(p)\n",
    "                paragraphs.append(par)\n",
    "\n",
    "            # print(paragraphs)\n",
    "\n",
    "            paragraphs = \"\\n\".join(paragraphs)\n",
    "            response = run_rag(paragraphs, question, options)\n",
    "\n",
    "            if response != default_response:\n",
    "                responses[f\"test_{test_idx}\"][\"node\"][f\"{index}_{i}_node\"] = {\"perturbation\": perturbation, \"paragraphs\": paragraphs, \"answer\": response, \"changed\": 1}\n",
    "                node_counter += 1\n",
    "\n",
    "                label_list.append(perturbation[-1])\n",
    "                node_positions.append(i/len(node_perturbations))\n",
    "                \n",
    "            else:\n",
    "\n",
    "                responses[f\"test_{test_idx}\"][\"node\"][f\"{index}_{i}_node\"] = {\"perturbation\": perturbation, \"paragraphs\": paragraphs, \"answer\": response, \"changed\": 0}\n",
    "\n",
    "\n",
    "            # print(perturbation)\n",
    "            # print(f\"Response: {response}\")\n",
    "            # print(f\"Original Answer: {answer}\")\n",
    "            # print(\"*\"*30)\n",
    "            # print()\n",
    "\n",
    "        responses[f\"test_{test_idx}\"][\"node\"][\"perturbation_amount\"] = len(node_perturbations)\n",
    "\n",
    "        # print(\"#\"*40)\n",
    "\n",
    "        # print(f\"Edge perturbations\")\n",
    "\n",
    "        for i, perturbation in enumerate(edge_perturbations):\n",
    "            temp_paths = paths[:]\n",
    "            temp_paths.pop(index)\n",
    "            \n",
    "            temp_paths.insert(index, perturbation)\n",
    "\n",
    "            # print(temp_paths)\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in temp_paths:\n",
    "                par = generate_pseudo_paragraph(p)\n",
    "                paragraphs.append(par)\n",
    "\n",
    "            # print(paragraphs)\n",
    "\n",
    "            paragraphs = \"\\n\".join(paragraphs)\n",
    "            response = run_rag(paragraphs, question, options)\n",
    "\n",
    "            if response != default_response:\n",
    "                responses[f\"test_{test_idx}\"][\"edge\"][f\"{index}_{i}_edge\"] = {\"perturbation\": perturbation, \"paragraphs\": paragraphs, \"answer\": response, \"changed\": 1}\n",
    "                edge_counter += 1\n",
    "\n",
    "                edge_positions.append(i/len(edge_perturbations))\n",
    "            \n",
    "            else:\n",
    "\n",
    "                responses[f\"test_{test_idx}\"][\"edge\"][f\"{index}_{i}_edge\"] = {\"perturbation\": perturbation, \"paragraphs\": paragraphs, \"answer\": response, \"changed\": 0}\n",
    "\n",
    "            \n",
    "            # print(perturbation)\n",
    "            # print(f\"Response: {response}\")\n",
    "            # print(f\"Original Answer: {answer}\")\n",
    "            # print(\"*\"*30)\n",
    "            # print()\n",
    "\n",
    "        responses[f\"test_{test_idx}\"][\"edge\"][\"perturbation_amount\"] = len(edge_perturbations)\n",
    "\n",
    "        # print(\"#\"*40)\n",
    "\n",
    "        # print(f\"Sub-path perturbations\")\n",
    "\n",
    "        for i, perturbation in enumerate(subpath_perturbations):\n",
    "            temp_paths = paths[:]\n",
    "            temp_paths.pop(index)\n",
    "            \n",
    "            temp_paths.insert(index, perturbation)\n",
    "\n",
    "            # print(temp_paths)\n",
    "\n",
    "            paragraphs = []\n",
    "            for p in temp_paths:\n",
    "                par = generate_pseudo_paragraph(p)\n",
    "                paragraphs.append(par)\n",
    "\n",
    "            # print(paragraphs)\n",
    "\n",
    "            paragraphs = \"\\n\".join(paragraphs)\n",
    "            response = run_rag(paragraphs, question, options)\n",
    "\n",
    "            if response != default_response:\n",
    "                responses[f\"test_{test_idx}\"][\"subpath\"][f\"{index}_{i}_subpath\"] = {\"perturbation\": perturbation, \"paragraphs\": paragraphs, \"answer\": response, \"changed\": 1}\n",
    "                subpath_counter += 1\n",
    "\n",
    "                subpath_positions.append(i/len(subpath_perturbations))\n",
    "\n",
    "            else:\n",
    "                responses[f\"test_{test_idx}\"][\"subpath\"][f\"{index}_{i}_subpath\"] = {\"perturbation\": perturbation, \"paragraphs\": paragraphs, \"answer\": response, \"changed\": 0}\n",
    "\n",
    "\n",
    "            \n",
    "            # print(perturbation)\n",
    "            # print(f\"Response: {response}\")\n",
    "            # print(f\"Original Answer: {answer}\")\n",
    "            # print(\"*\"*30)\n",
    "            # print()\n",
    "\n",
    "        responses[f\"test_{test_idx}\"][\"subpath\"][\"perturbation_amount\"] = len(subpath_perturbations)\n",
    "\n",
    "    responses[f\"test_{test_idx}\"][\"node_counter\"] = node_counter\n",
    "    responses[f\"test_{test_idx}\"][\"edge_counter\"] = edge_counter\n",
    "    responses[f\"test_{test_idx}\"][\"subpath_counter\"] = subpath_counter\n",
    "    \n",
    "    responses[f\"test_{test_idx}\"][\"labels\"] = label_list\n",
    "    responses[f\"test_{test_idx}\"][\"relative_node_positions\"] = node_positions\n",
    "    responses[f\"test_{test_idx}\"][\"relative_edge_positions\"] = edge_positions\n",
    "    responses[f\"test_{test_idx}\"][\"relative_subpath_positions\"] = subpath_positions\n",
    "\n",
    "    if node_counter == 0 and edge_counter == 0 and subpath_counter == 0:\n",
    "        continue\n",
    "    else:\n",
    "        temp_path = \"../data/temp_results1.json\"\n",
    "        final_path = \"../results/medmcqa.json\"\n",
    "\n",
    "        with open(temp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(responses, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        os.replace(temp_path, final_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255c30c",
   "metadata": {},
   "source": [
    "Procude LLM call counts and token amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = {}\n",
    "\n",
    "for test_idx,example in tqdm(list(medmcqa_df.iterrows())[:1000], desc=\"Apply Perturbations\", total=len(list(medmcqa_df.iterrows())[:1000])):\n",
    "    question = example[\"question\"]\n",
    "    options = example[\"options\"]\n",
    "\n",
    "    default_response, paragraph_length = produce_default_answer(question, options)\n",
    "\n",
    "    path_lists = []\n",
    "    paths = []\n",
    "\n",
    "    entity_pairs = extract_entities(question)\n",
    "    entity_pairs = [(pair.entity1, pair.entity2) for pair in entity_pairs]\n",
    "\n",
    "    for pair in entity_pairs:\n",
    "        shortest_path = find_shortest_path(pair[0], pair[1])\n",
    "        if shortest_path:\n",
    "            path_lists.append(shortest_path[:3])\n",
    "            paths.append(shortest_path[3])\n",
    "\n",
    "    if len(paths) == 0:\n",
    "        continue\n",
    "\n",
    "    responses[f\"test_{test_idx}\"] = {}\n",
    "\n",
    "    all_perturbations_counter = 0\n",
    "\n",
    "    for i in range(len(paths)):\n",
    "        perturbations = generate_perturbations(path_lists[i][0], path_lists[i][1], path_lists[i][2])\n",
    "        all_perturbations_counter += len(perturbations[\"node_perturbations\"]) + len(perturbations[\"edge_perturbations\"]) + len(perturbations[\"subpath_perturbations\"])\n",
    "\n",
    "    responses[f\"test_{test_idx}\"] = {}\n",
    "    responses[f\"test_{test_idx}\"][\"llm_calls\"] = all_perturbations_counter\n",
    "    responses[f\"test_{test_idx}\"][\"total_tokens\"] = all_perturbations_counter * paragraph_length\n",
    "\n",
    "with open(r\"../results/medmcqa_calls_amount.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fab1d3",
   "metadata": {},
   "source": [
    "Clean/remove examples that did not trigger ANY change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f09b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../results/medmcqa.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "rows_to_remove = []\n",
    "\n",
    "for row in data:\n",
    "    n_counter = data[row][\"node_counter\"]\n",
    "    e_counter = data[row][\"edge_counter\"]\n",
    "    s_counter = data[row][\"subpath_counter\"]\n",
    "\n",
    "    if n_counter == 0 and e_counter == 0 and s_counter == 0:\n",
    "        rows_to_remove.append(row)\n",
    "\n",
    "for row in rows_to_remove:\n",
    "    data.pop(row)\n",
    "\n",
    "with open(r\"../cleaned_results/cleaned_results_medmcqa.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c7dfa",
   "metadata": {},
   "source": [
    "Number of Perturbation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../cleaned_results/cleaned_results_mmlu.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "print(f\"Number of Perturbation Examples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca265ef1",
   "metadata": {},
   "source": [
    "Calculate **Impact**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a075f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../cleaned_results/cleaned_results_mmlu.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "node_impactfulness = 0\n",
    "edge_impactfulness = 0\n",
    "subpath_impactfulness = 0\n",
    "\n",
    "for row in data:\n",
    "\n",
    "    n_counter = 0\n",
    "    e_counter = 0\n",
    "    s_counter = 0\n",
    "\n",
    "    for perturbation in data[row][\"node\"]:\n",
    "        try:\n",
    "            # print(data[row][\"node\"][perturbation][\"changed\"])\n",
    "            if data[row][\"node\"][perturbation][\"changed\"] == 1:\n",
    "                n_counter += 1\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for perturbation in data[row][\"edge\"]:\n",
    "        try:\n",
    "            if data[row][\"edge\"][perturbation][\"changed\"] == 1:\n",
    "                e_counter += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for perturbation in data[row][\"subpath\"]:\n",
    "        try:\n",
    "            if data[row][\"subpath\"][perturbation][\"changed\"] == 1:\n",
    "                s_counter += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    n_counter = n_counter/data[row][\"node\"][\"perturbation_amount\"]\n",
    "    e_counter = e_counter/data[row][\"edge\"][\"perturbation_amount\"]\n",
    "    s_counter = s_counter/data[row][\"subpath\"][\"perturbation_amount\"]\n",
    "\n",
    "    # print(n_counter)\n",
    "    # print(e_counter)\n",
    "    # print(s_counter)\n",
    "    # print()\n",
    "\n",
    "    if n_counter >= e_counter and n_counter >= s_counter:\n",
    "        node_impactfulness += 1\n",
    "    elif e_counter > n_counter and e_counter >= s_counter:\n",
    "        edge_impactfulness += 1\n",
    "    elif s_counter > n_counter and s_counter > e_counter:\n",
    "        subpath_impactfulness += 1\n",
    "\n",
    "print(f\"Node Impactfulness: {node_impactfulness}\")\n",
    "print(f\"Edge Impactfulness: {edge_impactfulness}\")\n",
    "print(f\"Sub-path Impactfulness: {subpath_impactfulness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e5fe0",
   "metadata": {},
   "source": [
    "Generate Position Distribution Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "with open(r\"../cleaned_results/cleaned_results_medmcqa.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "all_node_positions = []\n",
    "all_edge_positions = []\n",
    "all_subpath_positions = []\n",
    "\n",
    "for row in data:\n",
    "    r_node_positions = data[row][\"relative_node_positions\"]\n",
    "    r_edge_positions = data[row][\"relative_edge_positions\"]\n",
    "    r_subpath_positions = data[row][\"relative_subpath_positions\"]\n",
    "\n",
    "    all_node_positions.extend(r_node_positions)\n",
    "    all_edge_positions.extend(r_edge_positions)\n",
    "    all_subpath_positions.extend(r_subpath_positions)\n",
    "\n",
    "print(f\"Node poisitions: {all_node_positions}\")\n",
    "print(f\"Edge poisitions: {all_edge_positions}\")\n",
    "print(f\"Subpath poisitions: {all_subpath_positions}\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.histplot(all_node_positions, kde=True, bins=20, color='skyblue')\n",
    "# plt.title('Node Positions Distribution')\n",
    "plt.xlabel(\"Relative Position\", fontsize=20, fontweight='bold')\n",
    "plt.xticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.ylabel(\"Critical Change Count\", fontsize=20, fontweight='bold')\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"../medmcqa_plots_pdf/node_positions.pdf\")\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "sns.histplot(all_edge_positions, kde=True, bins=20, color='salmon')\n",
    "# plt.title('Edge Positions Distribution')\n",
    "plt.xlabel(\"Relative Position\", fontsize=20, fontweight='bold')\n",
    "plt.xticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "# plt.ylabel(\"Critical Change Count\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"../medmcqa_plots_pdf/edge_positions.pdf\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "sns.histplot(all_subpath_positions, kde=True, bins=20, color='lightgreen')\n",
    "# plt.title('Subpath Positions Distribution')\n",
    "plt.xlabel(\"Relative Position\", fontsize=20, fontweight='bold')\n",
    "plt.xticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "# plt.ylabel(\"Critical Change Count\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"../medmcqa_plots_pdf/subpath_positions.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bcad5",
   "metadata": {},
   "source": [
    "Generate Node Label Distribution Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff65dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../cleaned_results/cleaned_results_medmcqa.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "node_labels = []\n",
    "\n",
    "for row in data:\n",
    "    labels = data[row][\"labels\"]\n",
    "    node_labels.extend(labels)\n",
    "\n",
    "cleaned_labels = []\n",
    "\n",
    "for label in node_labels:\n",
    "    if label[-1] == \"s\" :\n",
    "        cleaned_labels.append(label[:-1])\n",
    "    elif label[-2:] == \"**\":\n",
    "        temp_label = label[:-2]\n",
    "        if temp_label[-1] == \"s\" :\n",
    "            cleaned_labels.append(temp_label[:-1])\n",
    "        else: \n",
    "            cleaned_labels.append(temp_label)\n",
    "\n",
    "    elif label != \"Label2\":\n",
    "        cleaned_labels.append(label)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=cleaned_labels, order=sorted(set(cleaned_labels)))  # Sorted for readability\n",
    "# plt.title(\"Distribution of Node Labels\")\n",
    "plt.xlabel(\"Node Label\", fontsize=25, fontweight='bold')\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks(rotation=45, fontsize=20, fontweight='bold')\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"../medmcqa_plots_pdf/node_label_distribution.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53697c",
   "metadata": {},
   "source": [
    "Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058b7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../cleaned_results/cleaned_results_mmlu.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "node_degrees = []\n",
    "\n",
    "for row in data:\n",
    "    current_test = data[row]\n",
    "    node_data = current_test[\"node\"]\n",
    "    \n",
    "    for perturbation in node_data:\n",
    "        current_perturbation = node_data[perturbation]\n",
    "\n",
    "        try:\n",
    "            if current_perturbation[\"changed\"] == 1:\n",
    "                node = current_perturbation[\"perturbation\"][-2]\n",
    "\n",
    "                # print(f'Important Node: {node}')\n",
    "                d = G.degree[node]\n",
    "\n",
    "                node_degrees.append(d)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# print(node_degrees)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(node_degrees, bins=30, kde=True, color='skyblue')\n",
    "# plt.title(\"Distribution of Node Degrees for Changed Perturbations\")\n",
    "plt.xlabel(\"Degree\", fontsize=25, fontweight='bold')\n",
    "plt.ylabel(\"Frequency\", fontsize=25, fontweight='bold')\n",
    "\n",
    "plt.xticks(fontsize=20, fontweight='bold')\n",
    "plt.yticks(fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(r\"../mmlu_plots_pdf/node_degree_distribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50a9e6",
   "metadata": {},
   "source": [
    "***Cases where important nodes had the maximum degree within the derived path (not used in experiments)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../cleaned_results/cleaned_results_mmlu.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "max_node_degrees_counter = 0\n",
    "total_perturbations = 0\n",
    "\n",
    "for row in data:\n",
    "    current_test = data[row]\n",
    "    node_data = current_test[\"node\"]\n",
    "\n",
    "    total_perturbations += 1\n",
    "\n",
    "\n",
    "    temp_node_degrees = []\n",
    "    temp_important_node_degrees = []\n",
    "    \n",
    "    for perturbation in node_data:\n",
    "        current_perturbation = node_data[perturbation]\n",
    "\n",
    "        try:\n",
    "            node = current_perturbation[\"perturbation\"][-2]\n",
    "            d = G.degree[node]\n",
    "\n",
    "            temp_node_degrees.append(d)\n",
    "\n",
    "            if current_perturbation[\"changed\"] == 1:\n",
    "                temp_important_node_degrees.append(d)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(temp_node_degrees)\n",
    "    print(temp_important_node_degrees)\n",
    "    print()\n",
    "\n",
    "    max_degree_of_path = max(temp_node_degrees)\n",
    "    \n",
    "    if max_degree_of_path in temp_important_node_degrees:\n",
    "        print(f\"Max degree {max_degree_of_path} is from an important node.\")\n",
    "        max_node_degrees_counter += 1\n",
    "\n",
    "print(f\"Total cases where max degree was an important node: {max_node_degrees_counter}\")\n",
    "print(f\"Ratio: {max_node_degrees_counter/total_perturbations * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a7217",
   "metadata": {},
   "source": [
    "**Distribution of important node ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc69e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../cleaned_results/cleaned_results_mmlu.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "important_node_ranking_scores = []\n",
    "\n",
    "for row in data:\n",
    "    current_test = data[row]\n",
    "    node_data = current_test[\"node\"]\n",
    "\n",
    "    temp_node_degrees = {}\n",
    "    temp_important_nodes = []\n",
    "    \n",
    "    for perturbation in node_data:\n",
    "        current_perturbation = node_data[perturbation]\n",
    "\n",
    "        try:\n",
    "            node = current_perturbation[\"perturbation\"][-2]\n",
    "            d = G.degree[node]\n",
    "\n",
    "            temp_node_degrees[node] = d\n",
    "\n",
    "            if current_perturbation[\"changed\"] == 1:\n",
    "                temp_important_nodes.append(node)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    ordered_nodes = [(node_name, temp_node_degrees[node_name]) for  node_name in temp_node_degrees.keys()]\n",
    "    ordered_nodes = sorted(ordered_nodes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    sorted_node_names = [name for name, _ in ordered_nodes]\n",
    "    N = len(sorted_node_names)\n",
    "\n",
    "    important_relative_ranks = {\n",
    "        node: sorted_node_names.index(node) / (N - 1)\n",
    "        for node in temp_important_nodes\n",
    "    }\n",
    "\n",
    "    for node, rel_rank in important_relative_ranks.items():\n",
    "        important_node_ranking_scores.append(rel_rank)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(important_node_ranking_scores, bins=20, kde=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# plt.title(\"Distribution of Important Node Relative Ranks\")\n",
    "plt.xlabel(\"Relative Rank (0 = highest degree, 1 = lowest)\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel(\"Frequency\", fontsize=25, fontweight='bold')\n",
    "\n",
    "plt.xticks(fontsize=15, fontweight='bold')\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"../mmlu_plots_pdf/important_node_relative_rank_distribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c14b624",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ea115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "edge_betweenness_df = pd.read_csv(r\"../data/edge_betweenness.csv\")\n",
    "\n",
    "def extract_surrounding_entities(path):\n",
    "    parts = re.split(r'->\\[(.*?)\\]->', path)\n",
    "\n",
    "    nodes_before_after_blank = []\n",
    "\n",
    "    for i in range(1, len(parts)-1, 2):  # i is index of relation\n",
    "        if parts[i].strip() == \"__\":\n",
    "            source_node = parts[i-1].strip()\n",
    "            target_node = parts[i+1].strip()\n",
    "            nodes_before_after_blank.append((source_node, target_node))\n",
    "\n",
    "    return nodes_before_after_blank[0]\n",
    "\n",
    "def extract_betweenness(entity_pair):\n",
    "    pair_str = str(entity_pair)\n",
    "\n",
    "    match = edge_betweenness_df[edge_betweenness_df[\"Edge\"] == pair_str]\n",
    "\n",
    "    if not match.empty:\n",
    "        return match[\"Betweenness\"].values[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562591ef",
   "metadata": {},
   "source": [
    "**Distribution of important edge ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "with open(r\"../cleaned_results/cleaned_results_mmlu.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "important_edge_ranking_scores = []\n",
    "\n",
    "for row in data:\n",
    "    current_test = data[row]\n",
    "    edge_data = current_test.get(\"edge\", {})\n",
    "\n",
    "    temp_edge_betweenness = {}\n",
    "    temp_important_edges = []\n",
    "\n",
    "    for perturbation in edge_data:\n",
    "        current_perturbation = edge_data[perturbation]\n",
    "        try:\n",
    "            path = current_perturbation[\"perturbation\"]\n",
    "            node_pair = extract_surrounding_entities(path)\n",
    "            temp_edge_b = extract_betweenness(node_pair)\n",
    "\n",
    "            if temp_edge_b is not None:\n",
    "                temp_edge_betweenness[node_pair] = temp_edge_b\n",
    "                if current_perturbation.get(\"changed\") == 1:\n",
    "                    temp_important_edges.append(node_pair)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    ordered_edges = [\n",
    "        (f\"{edge[0]},{edge[1]}\", temp_edge_betweenness[edge])\n",
    "        for edge in temp_edge_betweenness\n",
    "    ]\n",
    "    ordered_edges = sorted(ordered_edges, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # print(\"\\nEdge Betweenness Scores:\")\n",
    "    # for edge_name, betweenness in ordered_edges:\n",
    "    #     print(f\"{edge_name}: {betweenness:.6f}\")\n",
    "\n",
    "    # print(temp_important_edges)\n",
    "\n",
    "    sorted_edge_names = [name for name, _ in ordered_edges]\n",
    "    N = len(sorted_edge_names)\n",
    "\n",
    "    if N > 1:\n",
    "        for edge in temp_important_edges:\n",
    "            edge_str = f\"{edge[0]},{edge[1]}\"\n",
    "            if edge_str in sorted_edge_names:\n",
    "                rel_rank = sorted_edge_names.index(edge_str) / (N - 1)\n",
    "                important_edge_ranking_scores.append(rel_rank)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(important_edge_ranking_scores, bins=20, kde=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "\n",
    "# plt.title(\"Distribution of Important Edge Relative Ranks\")\n",
    "plt.xlabel(\"Relative Rank (0 = highest betweenness, 1 = lowest)\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel(\"Frequency\", fontsize=25, fontweight='bold')\n",
    "\n",
    "plt.xticks(fontsize=15, fontweight='bold')\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(r\"../mmlu_plots_pdf/important_edge_relative_rank_distribution.pdf\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6990061",
   "metadata": {},
   "source": [
    "**Distribution of important sub-path ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e431d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "with open(r\"../cleaned_results/cleaned_results_medmcqa.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "def get_original_path(test):\n",
    "    node_data = test[\"node\"]\n",
    "    \n",
    "    path_list = node_data[\"0_0_node\"][\"perturbation\"][0].split(\"->\")\n",
    "    missing_node = node_data[\"0_0_node\"][\"perturbation\"][1]\n",
    "    path_list[0] = missing_node\n",
    "\n",
    "    return \"->\".join(path_list)\n",
    "\n",
    "def extract_subpath_entities(original_path, path):\n",
    "    original_path_list = original_path.split(\"->\")\n",
    "    path_list = path.split(\"->\")\n",
    "\n",
    "    empty_node_indices = []\n",
    "\n",
    "    for index,element in enumerate(path_list):\n",
    "        if element == \"__\":\n",
    "            empty_node_indices.append(index)\n",
    "\n",
    "    if len(empty_node_indices) < 2:\n",
    "        return []\n",
    "\n",
    "    return (f'{original_path_list[empty_node_indices[0]]}',f'{original_path_list[empty_node_indices[1]]}')\n",
    "\n",
    "\n",
    "subpath_relative_ranking_scores = []\n",
    "\n",
    "for row in data:\n",
    "    current_test = data[row]\n",
    "    original_path = get_original_path(current_test)\n",
    "    subpath_data = current_test[\"subpath\"]\n",
    "\n",
    "    temp_subpath_scores = []\n",
    "    temp_changed_subpath_scores = []\n",
    "\n",
    "    for perturbation in subpath_data:\n",
    "        current_perturbation = subpath_data[perturbation]\n",
    "\n",
    "        try:\n",
    "            if current_perturbation[\"changed\"] in [0, 1]:\n",
    "                path = current_perturbation[\"perturbation\"]\n",
    "                node_pair = extract_subpath_entities(original_path, path)\n",
    "                if len(node_pair) < 2:\n",
    "                    continue\n",
    "\n",
    "                temp_edge_betweenness = extract_betweenness(node_pair)\n",
    "                degree1 = G.degree[node_pair[0]]\n",
    "                degree2 = G.degree[node_pair[1]]\n",
    "                subpath_score = temp_edge_betweenness / (degree1 + degree2)\n",
    "\n",
    "                temp_subpath_scores.append(subpath_score)\n",
    "\n",
    "                if current_perturbation[\"changed\"] == 1:\n",
    "                    temp_changed_subpath_scores.append(subpath_score)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(temp_subpath_scores) == 0:\n",
    "        continue\n",
    "\n",
    "    ranks = rankdata([-s for s in temp_subpath_scores], method='average')\n",
    "    N = len(ranks)\n",
    "\n",
    "    for score in temp_changed_subpath_scores:\n",
    "        indices = [i for i, val in enumerate(temp_subpath_scores) if val == score]\n",
    "        avg_rank = sum(ranks[i] for i in indices) / len(indices)\n",
    "        zero_based_rank = avg_rank - 1\n",
    "        rel_rank = zero_based_rank / (N - 1) if N > 1 else 0\n",
    "        subpath_relative_ranking_scores.append(rel_rank)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(subpath_relative_ranking_scores, bins=20, color='skyblue', edgecolor='black')\n",
    "# plt.title('Histogram of Relative Ranking Scores for Changed Subpaths')\n",
    "plt.xlabel('Relative Rank (0 = highest rank)', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=25, fontweight='bold')\n",
    "\n",
    "plt.xticks(fontsize=15, fontweight='bold')\n",
    "plt.yticks(fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(r\"../medmcqa_plots_pdf/important_subpath_relative_rank_distribution.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
