{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db21f325",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import faiss\n",
    "\n",
    "from config import prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61784466",
   "metadata": {},
   "source": [
    "**Create vector database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d40310",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"all-minilm:33m\")\n",
    "\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"Hello world\")))\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "with open(r\"../data/statpearls_chunks.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for row in tqdm(data, desc=\"Processing chunks\", total=len(data)):\n",
    "    temp_doc = Document(page_content=row[\"chunk_text\"], metadata={\"chunk_id\": row[\"_id\"], \"chunk_index\": row[\"chunk_index\"] ,\"source_filename\": row[\"source_filename\"]})\n",
    "    documents.append(temp_doc)\n",
    "\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "vector_store.save_local(r\"../data/med_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d909f73",
   "metadata": {},
   "source": [
    "**Load Vector Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c93051",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"all-minilm:33m\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    r\"../data/med_index\", embeddings, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fad04",
   "metadata": {},
   "source": [
    "MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../data/benchmark.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "mmlu_benchmark = data[\"mmlu\"]\n",
    "ground_truth_answers = []\n",
    "\n",
    "for row in mmlu_benchmark:\n",
    "    ground_truth_answers.append(mmlu_benchmark[row][\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808eafe1",
   "metadata": {},
   "source": [
    "MedMCQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a7849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"../data/benchmark.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "medmcqa_benchmark = data[\"medmcqa\"]\n",
    "ground_truth_answers = []\n",
    "\n",
    "for row in medmcqa_benchmark:\n",
    "    ground_truth_answers.append(medmcqa_benchmark[row][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a219575",
   "metadata": {},
   "source": [
    "MedMCQA - Test examples (same as MMLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = []\n",
    "\n",
    "for q in medmcqa_benchmark:\n",
    "    question = medmcqa_benchmark[q][\"question\"]\n",
    "    options = medmcqa_benchmark[q][\"options\"]\n",
    "    \n",
    "    test_examples.append((question, options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34423180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_one_word_perturbations(context: str) -> list[tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Generate one-word-removal perturbations from the given context. Used for exhaustive perturbations.\n",
    "\n",
    "    For each word in the context, the function removes it to create a perturbed version of the context.\n",
    "\n",
    "    Returns all generated perturbations in a list:\n",
    "        list[tuple[str, str, int]]: A list of tuples, each containing:\n",
    "            - str: Perturbed context with one word removed\n",
    "            - str: The removed word\n",
    "            - int: Index of the removed word\n",
    "    \"\"\"\n",
    "\n",
    "    words = extract_tokens(context)\n",
    "    perturbations = []\n",
    "\n",
    "    for index, _ in enumerate(words):\n",
    "        temp_words = words[:]\n",
    "        \n",
    "        removed_word = temp_words.pop(index)\n",
    "\n",
    "        perturbed_context = \" \".join(temp_words)\n",
    "        perturbed_context = add_newlines_before_documents(perturbed_context)\n",
    "\n",
    "        temp_perturbation = (perturbed_context, removed_word, index)\n",
    "        perturbations.append(temp_perturbation)\n",
    "\n",
    "    return perturbations\n",
    "\n",
    "def remove_word_span(context: str, span_size: int) -> list[tuple[str, str, int]]:\n",
    "    \"\"\"\n",
    "    Generate span-based word removal perturbations from the given context.\n",
    "\n",
    "    For each contiguous span of 'span_size' words, remove the span to create a perturbed version of the context.\n",
    "\n",
    "    Returns all generated perturbations in a list:\n",
    "        list[tuple[str, str, int]]: A list of tuples, each containing:\n",
    "            - str: Perturbed context with one span removed\n",
    "            - str: The removed span text\n",
    "            - int: The starting index of the removed span in the original word list\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    words = extract_tokens(context)\n",
    "    perturbations = []\n",
    "\n",
    "    for i in range(len(words) - span_size + 1):\n",
    "        temp_words = words[:i] + words[i + span_size:]\n",
    "        removed_words = words[i:i + span_size]\n",
    "\n",
    "        perturbed_context = \" \".join(temp_words)\n",
    "        perturbed_context = add_newlines_before_documents(perturbed_context)\n",
    "\n",
    "        temp_perturbation = (perturbed_context, \" \".join(removed_words), i)\n",
    "        perturbations.append(temp_perturbation)\n",
    "\n",
    "    return perturbations\n",
    "\n",
    "\n",
    "def extract_tokens(text):\n",
    "    doc = nlp(text)\n",
    "    words = [token.text for token in doc if not token.is_punct]\n",
    "\n",
    "    return words\n",
    "\n",
    "def add_newlines_before_documents(text):\n",
    "    updated_text = re.sub(r'(?<!^) (Chunk \\d+)', r'\\n\\n\\1', text)\n",
    "    return updated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef3866",
   "metadata": {},
   "source": [
    "**Exhaustive Perturbations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "\n",
    "examples = test_examples[:]\n",
    "results = {}\n",
    "\n",
    "for index, example in tqdm(enumerate(examples), desc=\"Processing Perturbations\", total=len(examples)):\n",
    "    context = vector_store.similarity_search(example[0], k=1)\n",
    "    context = [c.page_content for c in context]\n",
    "\n",
    "    context_text = \"\"\n",
    "    for index, c in enumerate(context):\n",
    "        context_text += f\"Chunk {index}: {c.capitalize()}\\n\"\n",
    "\n",
    "    chain = prompt | model\n",
    "    original_response = chain.invoke({\"paragraph\": context_text, \"question\": question, \"options\": options})[0]\n",
    "    print(f\"Response: {original_response}\")\n",
    "\n",
    "    perturbations = remove_word_span(context_text, 5)\n",
    "\n",
    "    for perturbation in perturbations:\n",
    "        perturbed_text = perturbation[0]\n",
    "        removed_token = perturbation[1]\n",
    "\n",
    "        temp_answer = chain.invoke({\"paragraph\": perturbed_text, \"question\": question, \"options\": options})[0]  \n",
    "\n",
    "        if original_response != temp_answer:\n",
    "            print(f\"Peturbed Text: {perturbed_text}\") \n",
    "            print(f\"Temp answer: {temp_answer} | Removed token: {removed_token}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28904f27",
   "metadata": {},
   "source": [
    "**Count LLM calls and Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(model=\"llama3.2:3b-instruct-fp16\", temperature=0)\n",
    "\n",
    "examples = test_examples[:]\n",
    "responses = {}\n",
    "\n",
    "for test_index, example in tqdm(enumerate(examples), desc=\"Processing Perturbations\", total=len(examples)):\n",
    "    context = vector_store.similarity_search(example[0], k=1)\n",
    "    context = [c.page_content for c in context]\n",
    "\n",
    "    context_text = \"\"\n",
    "    for index, c in enumerate(context):\n",
    "        context_text += f\"Chunk {index}: {c.capitalize()}\\n\"\n",
    "\n",
    "    paragraph_length = len(context_text.split())\n",
    "\n",
    "    perturbations = remove_word_span(context_text, 5)\n",
    "\n",
    "    responses[f\"test_{test_index}\"] = {}\n",
    "    responses[f\"test_{test_index}\"][\"llm_calls\"] = len(perturbations)\n",
    "    responses[f\"test_{test_index}\"][\"total_tokens\"] = len(perturbations) * paragraph_length\n",
    "\n",
    "with open(r\"../results/medmcqa_calls_amount_simple.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, indent=2, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
